{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a6cd5c",
   "metadata": {},
   "source": [
    "# Image Embeddings, Clustering, and Similarity Search\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. Loading and exploring the Fashion MNIST dataset\n",
    "2. Extracting image embeddings using a pre-trained DINO Vision Transformer\n",
    "3. Visualizing embeddings in 2D using UMAP\n",
    "4. Performing nearest-neighbor search on embeddings to retrieve similar images\n",
    "\n",
    "The goal is to understand how deep models represent images in feature space and how this enables clustering and similarity-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7442e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from umap import UMAP\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b500a10",
   "metadata": {},
   "source": [
    "## 1. Dataset: Fashion MNIST\n",
    "\n",
    "In this demo, we use the `Fashion MNIST` dataset, a modern alternative to the classic [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset traditionally used for handwriting recognition. \n",
    "\n",
    "Fashion MNIST contains 70,000 grayscale images, each 28x28 pixels, distributed across 10 different clothing categories. These images are small, detailed, and varied enough to challenge our model while being simple enough for straightforward processing and quick training times.\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "### 1.1 Dataset Structure\n",
    "\n",
    "The dataset is split into two parts:\n",
    "\n",
    "- **Training Set:** This includes `train_images` and `train_labels`. These are the arrays that our model will learn from. The model sees these images and their corresponding labels, adjusting its weights and biases to reduce classification error.\n",
    "\n",
    "- **Test Set:** This comprises `test_images` and `test_labels`. These are used to evaluate how well our model performs on data it has never seen before. This is crucial for understanding the model's generalization capability.\n",
    "\n",
    "### 1.2 Understanding the Data\n",
    "\n",
    "Each image in the dataset is a 28x28 NumPy array. The pixel values range from 0 to 255, with 0 being black, 255 being white, and the various shades of gray in between. The labels are integers from 0 to 9, each representing a specific category of clothing.\n",
    "\n",
    "**Class Names:**\n",
    "\n",
    "The dataset doesn't include the names of the clothing classes, so we will manually define them for clarity when visualizing our results. Here's the mapping:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "We will use a subset of the training set to build embeddings and visualize structure.\n",
    "\n",
    "## 2. Loading the dataset\n",
    "\n",
    "Let's load the Fashion MNIST dataset directly from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names for Fashion MNIST\n",
    "class_names = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# define transformation: convert to tensor and normalize\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "\n",
    "# load dataset and apply transformation\n",
    "train_data = load_dataset(\"fashion_mnist\", split=\"train\")\n",
    "print(\"Training set has {} instances\".format(len(train_data)))\n",
    "\n",
    "# convert dataset to tensors and store in memory for fast access\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for example in train_data:\n",
    "    img = transform(example[\"image\"])\n",
    "    label = example[\"label\"]\n",
    "    train_images.append(img)\n",
    "    train_labels.append(label)\n",
    "\n",
    "train_images = torch.stack(train_images)  # shape: (60000, 1, 28, 28)\n",
    "train_labels = torch.tensor(train_labels)  # shape: (60000,)\n",
    "\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze class distribution\n",
    "from collections import Counter\n",
    "\n",
    "train_list = [item[\"label\"] for item in train_data]\n",
    "label_counts = Counter(train_list)\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for i in range(10):\n",
    "    print(\n",
    "        f\"{class_names[i]:15s}: {label_counts[i]:5d} images ({label_counts[i] / len(train_data) * 100:.1f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9d3db",
   "metadata": {},
   "source": [
    "## 3. Understanding Pixel Values\n",
    "\n",
    "- Each image in the Fashion MNIST dataset is represented in grayscale with pixel values ranging from 0 to 255.\n",
    "\n",
    "- We applied Scaling and Normalization methods so that each pixel value is centered around 0 and falls within the range [-1, 1].\n",
    "\n",
    "- This normalization is often used in deep learning models as it centers the data around 0, which can lead to faster convergence during training. It can also help mitigate issues caused by different lighting and contrast in images.\n",
    "\n",
    "- The value `-1` represents black, `1` represents white, and the values in between represent various shades of gray.\n",
    "\n",
    "Let's inspect the first image in the training set displaying these pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting firt image\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0][0], cmap=\"gray\")  # first image in the dataset\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_color(value: float) -> str:\n",
    "    \"\"\"Returns 'white' for dark pixels and 'black' for light pixels.\"\"\"\n",
    "    return \"white\" if value < 0.5 else \"black\"\n",
    "\n",
    "\n",
    "image_numpy = train_images[0][0].squeeze().numpy()\n",
    "label = train_labels[0]\n",
    "\n",
    "# Plotting the image\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.imshow(image_numpy, cmap=\"gray\")\n",
    "plt.title(\n",
    "    class_names[label],\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Overlaying the pixel values\n",
    "for i in range(image_numpy.shape[0]):\n",
    "    for j in range(image_numpy.shape[1]):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            \"{:.1f}\".format(image_numpy[i, j]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=get_text_color(image_numpy[i, j]),\n",
    "        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i][0], cmap=\"gray\")\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0d30b",
   "metadata": {},
   "source": [
    "## 5. Image Embeddings\n",
    "\n",
    "Deep learning models do not compare images at the level of raw pixels. Instead, they convert images into **embeddings** — numerical feature vectors that capture the visual characteristics of the image.\n",
    "\n",
    "### What are Image Embeddings?\n",
    "\n",
    "An embedding is a vector representation where:\n",
    "\n",
    "- Images that are visually similar have **similar embeddings**\n",
    "- Images that are visually different have **distant embeddings**\n",
    "\n",
    "This allows us to measure similarity between images using vector distances (e.g., cosine similarity).\n",
    "\n",
    "### Why Use a Pretrained Model?\n",
    "\n",
    "Training a feature extractor from scratch requires large datasets and compute. Instead, we use a **pretrained vision transformer**, which has learned general visual patterns such as edges, textures, shapes, and object parts.\n",
    "\n",
    "### [DINOv2: A Self-supervised Vistion Transformer Model](https://dinov2.metademolab.com/)\n",
    "\n",
    "We will use **DINOv2**, a self-supervised vision transformer model. It learns useful visual representations **without labels**, making it a strong general-purpose feature extractor.\n",
    "\n",
    "- Input: an image\n",
    "- Output: a fixed-length embedding (e.g., a 768-dimensional vector)\n",
    "- Benefit: embeddings can be used for visualization, clustering, and similarity search\n",
    "\n",
    "We now load DINOv2 and extract embeddings for our Fashion MNIST images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_name = \"facebook/dinov2-base\"\n",
    "\n",
    "# load processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModel.from_pretrained(model_name).eval().to(device)\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "print(\"Embedding dimension:\", model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200058c8",
   "metadata": {},
   "source": [
    "## 6. Extracting Image Embeddings\n",
    "\n",
    "Fashion MNIST images are 28×28 grayscale images. DINOv2 expects RGB images at a higher resolution.  \n",
    "We convert each image to RGB and allow the model’s image processor to handle resizing and normalization.\n",
    "\n",
    "To avoid memory issues, embeddings are extracted in batches.  \n",
    "Each image is mapped to a single embedding vector (here, 768-dimensional for DINOv2 Base).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1abe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, images: torch.Tensor, batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract embeddings from a batch of images using DINOv2.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained DINOv2 model\n",
    "        images: Tensor of shape (N, 1, 28, 28)\n",
    "        batch_size: Number of images per batch for the model\n",
    "\n",
    "    Returns:\n",
    "        numpy array of shape (N, embedding_dim)\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i : i + batch_size]\n",
    "\n",
    "            # convert tensor grayscale -> PIL RGB list\n",
    "            pil_batch = [\n",
    "                Image.fromarray((img.squeeze().numpy() * 255).astype(np.uint8)).convert(\n",
    "                    \"RGB\"\n",
    "                )\n",
    "                for img in batch\n",
    "            ]\n",
    "\n",
    "            # preprocess for model\n",
    "            inputs = processor(images=pil_batch, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # extract CLS token representation\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(cls_embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b20659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample a subset of images\n",
    "num_samples = 5000\n",
    "indices = np.random.choice(len(train_images), size=num_samples, replace=False)\n",
    "\n",
    "# extract subset of images and labels\n",
    "subset_images = train_images[indices]\n",
    "subset_labels = train_labels[indices]\n",
    "\n",
    "print(\"Extracting embeddings...\")\n",
    "embeddings = extract_embeddings(model, subset_images, batch_size=64)\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e15dbd",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction with UMAP\n",
    "\n",
    "The embeddings produced by DINOv2 are 768-dimensional vectors.  \n",
    "To understand how images relate to one another, we project these embeddings into **two dimensions** for visualization.\n",
    "\n",
    "### UMAP\n",
    "\n",
    "We use **[UMAP (Uniform Manifold Approximation and Projection)](https://umap-learn.readthedocs.io/en/latest/)**, a method that:\n",
    "- Reduces high-dimensional data to 2D\n",
    "- Preserves neighborhood structure (similar items stay close)\n",
    "- Works efficiently on large datasets\n",
    "\n",
    "If the embeddings capture meaningful visual structure, images from the same clothing class should form clusters in the 2D projection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce 768D embeddings to 2D for visualization\n",
    "umap_reducer = UMAP(\n",
    "    n_components=2,\n",
    "    metric=\"cosine\",\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# fit and transform\n",
    "embeddings_2d = umap_reducer.fit_transform(embeddings)\n",
    "print(\"UMAP projection shape:\", embeddings_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization of clusters\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=200)\n",
    "\n",
    "# create a color for each class\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# overlay colored points by class\n",
    "for class_id in range(10):\n",
    "    mask = subset_labels == class_id\n",
    "    ax.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[colors[class_id]],\n",
    "        label=class_names[class_id],\n",
    "        alpha=0.7,\n",
    "        s=30,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=11)\n",
    "ax.set_title(\n",
    "    \"UMAP Projection of DINOv2 Image Embeddings\",\n",
    "    fontsize=18,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"UMAP Dimension 1\", fontsize=14)\n",
    "ax.set_ylabel(\"UMAP Dimension 2\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f1465",
   "metadata": {},
   "source": [
    "## 8. Evaluating Cluster Quality\n",
    "\n",
    "Once the embeddings are projected to 2D, we can assess how well the points form clusters.  \n",
    "We use the **Silhouette Score**, which measures how similar a sample is to its own class compared to other classes.\n",
    "\n",
    "### Silhouette Score Interpretation\n",
    "\n",
    "| Score Range | Interpretation |\n",
    "|------------|----------------|\n",
    "| 0.50 – 1.00 | Clear, well-separated clusters |\n",
    "| 0.26 – 0.49 | Some clustering structure with overlap |\n",
    "| 0.00 – 0.25 | Weak clustering or heavy overlap |\n",
    "| < 0 | Points may be assigned to the wrong cluster |\n",
    "\n",
    "Fashion MNIST typically falls in the **0.25–0.45** range.  \n",
    "Some classes (e.g., *Sneaker*, *Bag*) are visually distinct, while others (e.g., *T-shirt/top* vs *Shirt*) look similar, leading to overlapping clusters.\n",
    "\n",
    "The Silhouette Score provides a single numeric summary, but the visualization remains the primary way to interpret how the model organizes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after UMAP, check cluster quality\n",
    "# Silhouette score: how well the projected points form clusters\n",
    "score = silhouette_score(embeddings_2d, subset_labels, metric=\"euclidean\")\n",
    "print(f\"Silhouette Score: {score:.3f}\")\n",
    "print(\"(Higher is better, range: -1 to 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42155a9b",
   "metadata": {},
   "source": [
    "## 9. Similarity Search with Nearest Neighbors\n",
    "\n",
    "Because each image is now represented as an embedding vector, we can measure similarity between images by comparing these vectors.\n",
    "\n",
    "We use **K-Nearest Neighbors (KNN)** to retrieve the closest images in embedding space:\n",
    "\n",
    "- The **query image** is compared to all others.\n",
    "- We compute distances between embedding vectors.\n",
    "- The top matches are returned as the most similar images.\n",
    "\n",
    "This provides a simple way to perform:\n",
    "- Image retrieval\n",
    "- Recommendation based on visual similarity\n",
    "- Non-parametric classification (no training required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit nearest neighbors model on embeddings\n",
    "nn_model = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "nn_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nearest_neighbors(query_index: int, k: int = 6):\n",
    "    \"\"\"\n",
    "    Display a query image and its nearest neighbors in embedding space.\n",
    "    \"\"\"\n",
    "    k += 1  # include query image\n",
    "    distances, indices = nn_model.kneighbors([embeddings[query_index]], n_neighbors=k)\n",
    "\n",
    "    # set figure width proportional to k\n",
    "    fig_width = max(8, k * 2)  # scales nicely for k=6, 8, 10, ...\n",
    "    fig, axes = plt.subplots(1, k, figsize=(fig_width, 3), dpi=150)\n",
    "\n",
    "    # ensure axes is iterable\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Query Image and Top-5 Most Similar Images\", fontsize=14, weight=\"bold\", y=1.05\n",
    "    )\n",
    "\n",
    "    for i, (ax, idx) in enumerate(zip(axes, indices[0])):\n",
    "        img = subset_images[idx][0].cpu().numpy()\n",
    "        label = class_names[subset_labels[idx]]\n",
    "\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Query\\n({label})\", fontsize=10, weight=\"bold\")\n",
    "        else:\n",
    "            similarity = 1 - distances[0][i]\n",
    "            ax.set_title(f\"{label}\\nSim={similarity:.2f}\", fontsize=9)\n",
    "\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # reserve extra vertical space for suptitle\n",
    "    fig.subplots_adjust(top=1, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69424cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random test image from the subset\n",
    "query = np.random.randint(0, num_samples)\n",
    "\n",
    "print(\"Query image class:\", class_names[subset_labels[query]])\n",
    "show_nearest_neighbors(query, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_visualize_class(class_name, sample_labels=subset_labels, k: int = 5):\n",
    "    \"\"\"\n",
    "    Find random image from a class and show similar images\n",
    "    \"\"\"\n",
    "    # Get class id\n",
    "    class_name = class_name.capitalize()  # ensure proper capitalization\n",
    "    class_id = class_names.index(class_name)\n",
    "\n",
    "    # Find images of this class in our sample\n",
    "    class_indices = np.where(sample_labels == class_id)[0]\n",
    "\n",
    "    if len(class_indices) == 0:\n",
    "        print(f\"No {class_name} found in sample!\")\n",
    "        return\n",
    "\n",
    "    # Pick random image\n",
    "    query_idx = random.choice(class_indices)\n",
    "\n",
    "    print(f\"Finding images similar to a {class_name}...\")\n",
    "    show_nearest_neighbors(query_idx, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eda3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out! change the class name below\n",
    "find_and_visualize_class(\"sneaker\", k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2aa130",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### Embeddings\n",
    "- Deep models represent images as numerical vectors (embeddings).\n",
    "- Images that are visually or semantically similar have embeddings that are close in vector space.\n",
    "- These representations capture structure beyond raw pixel similarity.\n",
    "\n",
    "### Clustering in Embedding Space\n",
    "- Without any additional training, similar items naturally form clusters.\n",
    "- This occurs because the pretrained model has learned general visual features from large-scale image data.\n",
    "- Clothing categories in Fashion MNIST occupy distinct regions in embedding space, with some expected overlap between visually similar classes.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "- UMAP provides a way to project high-dimensional embeddings into a 2D space for visualization.\n",
    "- The 2D visualization reflects underlying structure, though some information is lost in the projection.\n",
    "- Cluster separation can be evaluated quantitatively with the Silhouette Score.\n",
    "\n",
    "### Similarity Search with Nearest Neighbors\n",
    "- Nearest neighbor search allows us to retrieve images that are similar to a query image.\n",
    "- This demonstrates how embeddings can support tasks such as classification, recommendation, and search without training a model.\n",
    "\n",
    "### Possible Extensions\n",
    "1. Try different variants of DINO (e.g., ViT-S, ViT-B, ViT-L) and compare embedding quality.\n",
    "2. Experiment with UMAP parameters (e.g., `n_neighbors`, `min_dist`) to adjust cluster compactness.\n",
    "3. Compare UMAP with PCA or t-SNE for dimensionality reduction.\n",
    "4. Implement K-Nearest Neighbor classification directly on embeddings.\n",
    "5. Apply the pipeline to a different dataset (e.g., CIFAR-10) or your own images.\n",
    "\n",
    "This pipeline demonstrates how pretrained vision models enable visualization, clustering, and similarity-based retrieval directly from image embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd845ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
